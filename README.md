
<div align="right">
  <details>
    <summary >🌐 Language</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=QingyangZhang&project=Label-Free-RLVR&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=QingyangZhang&project=Label-Free-RLVR&lang=zh-CN">简体中文</a>
        | <a href="https://openaitx.github.io/view.html?user=QingyangZhang&project=Label-Free-RLVR&lang=zh-TW">繁體中文</a>
        | <a href="https://openaitx.github.io/view.html?user=QingyangZhang&project=Label-Free-RLVR&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=QingyangZhang&project=Label-Free-RLVR&lang=ko">한국어</a>
        | <a href="https://openaitx.github.io/view.html?user=QingyangZhang&project=Label-Free-RLVR&lang=hi">हिन्दी</a>
        | <a href="https://openaitx.github.io/view.html?user=QingyangZhang&project=Label-Free-RLVR&lang=th">ไทย</a>
        | <a href="https://openaitx.github.io/view.html?user=QingyangZhang&project=Label-Free-RLVR&lang=fr">Français</a>
        | <a href="https://openaitx.github.io/view.html?user=QingyangZhang&project=Label-Free-RLVR&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=QingyangZhang&project=Label-Free-RLVR&lang=es">Español</a>
        | <a href="https://openaitx.github.io/view.html?user=QingyangZhang&project=Label-Free-RLVR&lang=it">Itapano</a>
        | <a href="https://openaitx.github.io/view.html?user=QingyangZhang&project=Label-Free-RLVR&lang=ru">Русский</a>
        | <a href="https://openaitx.github.io/view.html?user=QingyangZhang&project=Label-Free-RLVR&lang=pt">Português</a>
        | <a href="https://openaitx.github.io/view.html?user=QingyangZhang&project=Label-Free-RLVR&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=QingyangZhang&project=Label-Free-RLVR&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=QingyangZhang&project=Label-Free-RLVR&lang=ar">العربية</a>
        | <a href="https://openaitx.github.io/view.html?user=QingyangZhang&project=Label-Free-RLVR&lang=fa">فارسی</a>
        | <a href="https://openaitx.github.io/view.html?user=QingyangZhang&project=Label-Free-RLVR&lang=tr">Türkçe</a>
        | <a href="https://openaitx.github.io/view.html?user=QingyangZhang&project=Label-Free-RLVR&lang=vi">Tiếng Việt</a>
        | <a href="https://openaitx.github.io/view.html?user=QingyangZhang&project=Label-Free-RLVR&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div>

# Awesome Label-Free Reinforcement Learning with Verifiable Rewards

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

A curated collection of papers on Label-Free Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Models (LLMs).

> **By [Qingyang Zhang](qingyangzhang.github.io), [Haitao Wu](https://haitaowutju.github.io) and [Yi Ding](https://dripnowhy.github.io). If there are any papers I missed, please let me know!**


## Table of Contents
- [Awesome Label-Free Reinforcement Learning with Verifiable Rewards](#awesome-label-free-reinforcement-learning-with-verifiable-rewards)
  - [Table of Contents](#table-of-contents)
  - [Overview](#overview)
    - [Before DeepSeek-R1-Zero](#before-deepseek-r1-zero)
    - [RLVR without External Supervision](#rlvr-without-external-supervision)
    - [RLVR with Limited Data](#rlvr-with-limited-data)
    - [Others](#others)
  - [Star History](#star-history)

## Overview

### Before DeepSeek-R1-Zero

[Preference Optimization for Reasoning with Pseudo Feedback](https://arxiv.org/abs/2411.16345), ArXiv, 2024-11, ICLR'25 spotlight

[Self-Consistency Preference Optimization](https://arxiv.org/abs/2411.04109), ArXiv, 2024-11

### RLVR without External Supervision

[Right question is already half the answer: Fully unsupervised LLM reasoning incentivization](https://arxiv.org/abs/2504.05812), ArXiv, 2025-04-08

[Ttrl: Test-time reinforcement learning](https://arxiv.org/abs/2504.16084), ArXiv, 2025-04-22

[Absolute Zero: Reinforced Self-play Reasoning with Zero Data](https://arxiv.org/abs/2505.03335), ArXiv, 2025-05-06

[The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning](https://arxiv.org/abs/2505.15134), ArXiv, 2025-05-21

[SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation](https://arxiv.org/abs/2505.16637), ArXiv, 2025-05-22

[SeRL: Self-Play Reinforcement Learning for Large Language Models with Limited Data](https://arxiv.org/abs/2505.20347), ArXiv, 2025-05-25

[Learning to Reason without External Rewards](https://arxiv.org/abs/2505.19590), ArXiv, 2025-05-26

[Surrogate Signals from Format and Length: Reinforcement Learning for Solving Mathematical Problems without Ground Truth Answers](https://arxiv.org/abs/2505.19439), ArXiv, 2025-05-26

[Spurious Rewards: Rethinking Training Signals in RLVR](https://github.com/ruixin31/Rethink_RLVR/tree/main?tab=readme-ov-file), Blog, 2025-05-27

[Can Large Reasoning Models Self-Train?](https://arxiv.org/abs/2505.21444), ArXiv, 2025-05-27

[Maximizing Confidence Alone Improves Reasoning](https://arxiv.org/abs/2505.22660), ArXiv, 2025-05-28

[Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO](https://arxiv.org/abs/2505.22453v1), ArXiv, 2025-05-29

[ZeroGUI: Automating Online GUI Learning at Zero Human Cost](https://arxiv.org/abs/2505.23762), ArXiv, 2025-05-29

[Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.08745), ArXiv, 2025-06-02

[Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models](https://arxiv.org/abs/2506.06395v1), ArXiv, 2025-06-05

[Self-Adapting Language Models](https://arxiv.org/abs/2506.10943), ArXiv, 2025-06-12

[No Free Lunch: Rethinking Internal Feedback for LLM Reasoning](https://arxiv.org/abs/2506.17219), ArXiv, 2025-06-20

### RLVR with Limited Data

[Self-rewarding correction for mathematical reasoning](https://arxiv.org/pdf/2502.19613), ArXiv, 2025-02-26

[Reinforcement Learning for Reasoning in Large Language Models with One Training Example](https://arxiv.org/abs/2504.20571), ArXiv, 2025-04-29

[Evolving LLMs’ Self-Refinement Capability via Iterative Preference Optimization](https://arxiv.org/pdf/2502.05605), ArXiv, 2025-05-17

[Sherlock: Self-Correcting Reasoning in Vision-Language Models](https://arxiv.org/pdf/2505.22651), ArXiv, 2025-05-28

[Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models](https://arxiv.org/abs/2506.06395), Arxiv, 2025-07-05 

### Others

[SLOT: Sample-specific Language Model Optimization at Test-time](https://arxiv.org/abs/2505.12392), ArXiv, 2025-05-18

[One-shot Entropy Minimization](https://arxiv.org/abs/2505.20282), ArXiv, 2025-05-26

[Reinforcing General Reasoning without Verifiers](https://arxiv.org/abs/2505.21493), ArXiv, 2025-05-27

[Incorrect Baseline Evaluations Call into Question Recent LLM-RL Claims](https://safe-lip-9a8.notion.site/Incorrect-Baseline-Evaluations-Call-into-Question-Recent-LLM-RL-Claims-2012f1fbf0ee8094ab8ded1953c15a37#2022f1fbf0ee80cb9b18f7eac460410a), Blog, 2025-05-29
> A critical review on RLVR evaluation setups.

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=QingyangZhang/Label-Free-RLVR&Date&type=Date)](https://www.star-history.com/#QingyangZhang/Label-Free-RLVR&Date&Date)
